{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aff-Wild",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBuLNM7uL1Fc",
        "colab_type": "text"
      },
      "source": [
        "**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02_Sz0K54GvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "slim = tf.contrib.slim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_6ngXC8gAkD",
        "colab_type": "text"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpfya5wb_qDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUkgAoNf1jI",
        "colab_type": "text"
      },
      "source": [
        "Define location of project file in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npk5P2OQBqGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_affwild_path = '/content/drive/My Drive/aff_wild'\n",
        "inputcsv_path = os.path.join(drive_affwild_path, 'input.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtenvksifxF6",
        "colab_type": "text"
      },
      "source": [
        "Append path to files in drive to allow importing of personal python modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMCi6aKaUmmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append(sys.path.append(drive_affwild_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfh4XXA1bzFQ",
        "colab_type": "text"
      },
      "source": [
        "**Pretrained model locations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urt57fpKQ0Rp",
        "colab_type": "text"
      },
      "source": [
        "Model | CCC A | CCC V | MSE A | MSE V\n",
        "--- | --- | --- | --- | ---\n",
        "affwildnet_vggface_gru | 0.7251 | 0.8107 | 0.0344 | 0.0600\n",
        "affwildnet_resnet_gru | 0.7009 | 0.7760 | 0.0346 | 0.0615\n",
        "vggface_4096 | **0.8013** | **0.8486** | **0.0266** | **0.0454**\n",
        "vggface_2000 | 0.7881 | 0.8410 | 0.0299 | 0.0497"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeQPn1VDbzeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "affwildnet_vggface_path = os.path.join(drive_affwild_path, 'affwildnet', 'affwildnet-vggface-gru', 'model.ckpt-0')\n",
        "affwildnet_resnet_path  = os.path.join(drive_affwild_path, 'affwildnet', 'affwildnet-resnet-gru', 'model.ckpt-0')\n",
        "vggface_2000_path       = os.path.join(drive_affwild_path, 'vggface', '4096x2000x2', 'model.ckpt-0')\n",
        "vggface_4096_path       = os.path.join(drive_affwild_path, 'vggface', '4096x4096x2', 'model.ckpt-975')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPNP3dWxKogi",
        "colab_type": "text"
      },
      "source": [
        "**Defining tf cli flags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlDFvfLiKXwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = tf.app.flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHtHaTmIKusD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'batch_size' in FLAGS.__flags.keys():\n",
        "    del FLAGS.batch_size\n",
        "tf.app.flags.DEFINE_integer('batch_size', 1, 'The batch size to use.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9yHauSyKwqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'seq_length' in FLAGS.__flags.keys():\n",
        "    del FLAGS.seq_length\n",
        "tf.app.flags.DEFINE_integer('seq_length', 80, \n",
        "  'the sequence length: how many consecutive frames to use for the RNN; if the network is only CNN then put here any number you want : total_batch_size = batch_size * seq_length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC-ysuITKy1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'size' in FLAGS.__flags.keys():\n",
        "    del FLAGS.size\n",
        "tf.app.flags.DEFINE_integer('size', 96, 'dimensions of input images, e.g. 96x96')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc3nTY4KKzMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'network' in FLAGS.__flags.keys():\n",
        "    del FLAGS.network\n",
        "tf.app.flags.DEFINE_string('network',  'affwildnet_resnet', \n",
        "  'which network architecture we want to use,  pick between : vggface_4096, vggface_2000, affwildnet_vggface, affwildnet_resnet')                           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJqkUcySKzTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'input_file' in FLAGS.__flags.keys():\n",
        "    del FLAGS.input_file\n",
        "tf.app.flags.DEFINE_string('input_file',  inputcsv_path, \n",
        "  'the input file : it should be in the format: image_file_location,valence_value,arousal_value  and images should be jpgs')                           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTctgGIQLtlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'pretrained_model_checkpoint_path' in FLAGS.__flags.keys():\n",
        "    del FLAGS.pretrained_model_checkpoint_path\n",
        "tf.app.flags.DEFINE_string('pretrained_model_checkpoint_path', affwildnet_resnet_path,\n",
        "                           '''the pretrained model checkpoint path to restore,if there exists one  '''\n",
        "                           '''''')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1s5gW_HSDCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fixes an error related to using tf.app.flags in jupyter notebooks\n",
        "sys.argv = \"\".split(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN8ntbMlBKoB",
        "colab_type": "text"
      },
      "source": [
        "**data_process.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXHB0IiMBPM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_labeled_image_list(image_list_file):\n",
        "    \"\"\"Reads a .csv file containing paths and labels, should be in the format:\n",
        "      image_file_location1,valence_value1,arousal_value1\n",
        "      image_file_location2,valence_value2,arousal_value2\n",
        "      ...\n",
        "        images should be jpgs\n",
        "          Returns:\n",
        "          a list with all filenames in file image_list_file and a list containing lists of the 2 respective labels  \n",
        "    \"\"\"\n",
        "    f = open(image_list_file, 'r')\n",
        "    filenames = []\n",
        "\n",
        "    labels_val = []\n",
        "    labels_ar = []\n",
        "\n",
        "    for line in f:\n",
        "        inputs = line.rstrip().split(',')\n",
        "        filenames.append(inputs[0])\n",
        "        labels_val.append(float(inputs[1]))\n",
        "        labels_ar.append(float(inputs[2]))\n",
        "    \n",
        "\n",
        "    labels = [list(a) for a in zip(labels_val, labels_ar)]\n",
        "    return filenames, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB4zBmxpBPTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decodeRGB(input_queue,seq_length,size=96):\n",
        "    \"\"\" Args:\n",
        "          filename_and_label_tensor: A scalar string tensor.\n",
        "          Returns:\n",
        "          Three tensors: one with the decoded images, one with the corresponding labels and another with the image file locations\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = input_queue[1]\n",
        "    images_locations = input_queue[2]\n",
        "\n",
        "    for i in range(seq_length):\n",
        "      file_content = tf.read_file(input_queue[0][i])\n",
        "      image = tf.image.decode_jpeg(file_content, channels=3)\n",
        "      image = tf.image.resize_images(image, tf.convert_to_tensor([size,size]))\n",
        "      images.append(image)\t\n",
        "\n",
        "    return images,labels,images_locations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATg_CxA9BPYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_rnn_input_per_seq_length_size(images,labels,seq_length):\n",
        "\t\"\"\"\n",
        "        Args:\n",
        "        images : the images file locations with shape (N,1) where N is the total number of images\n",
        "        labels: the corresponding labels with shape (N,2) where N is the total number of images\n",
        "        seq_length: the sequence length that we want\n",
        "        Returns:\n",
        "        Two tensors: the images file locations with shape ( int(N/80),80 ) and corresponding labels with shape ( int(N/80),80,2 )\n",
        "\t\"\"\"\n",
        "\tims =[]\n",
        "\tlabs = []\n",
        "\tfor l in range(int(len(images)/seq_length)):   \n",
        "\t        a = images[int(l)*seq_length:int(l)*seq_length+seq_length]\n",
        "\t        b = labels[int(l)*seq_length:int(l)*seq_length+seq_length]\n",
        "\t        ims.append(a)\n",
        "\t        labs.append(b)\n",
        "   \n",
        "\treturn ims,labs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylK3UHOcMFaE",
        "colab_type": "text"
      },
      "source": [
        "**Concordance calculation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ1pfuEsKIH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concordance_cc2(r1, r2):\n",
        "     mean_cent_prod = ((r1 - r1.mean()) * (r2 - r2.mean())).mean()\n",
        "     return (2 * mean_cent_prod) / (r1.var() + r2.var() + (r1.mean() - r2.mean()) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXGQmNCQL6Qf",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijAEzMcsKLgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate():\n",
        "    g = tf.Graph()\n",
        "    with g.as_default():\n",
        "\n",
        "        image_list, label_list = read_labeled_image_list(FLAGS.input_file)\n",
        "        # split into sequences, note: in the cnn models case this is splitting into batches of length: seq_length ;\n",
        "        #                             for the cnn-rnn models case, I do not check whether the images in a sequence are consecutive or the images are from the same video/the images are displaying the same person \n",
        "        image_list, label_list = make_rnn_input_per_seq_length_size(image_list,label_list,FLAGS.seq_length)\n",
        "\n",
        "        images = tf.convert_to_tensor(image_list)\n",
        "        labels = tf.convert_to_tensor(label_list)\n",
        "\n",
        "        # Makes an input queue\n",
        "        input_queue = tf.train.slice_input_producer([images, labels,images],num_epochs=None, shuffle=False, seed=None,capacity=1000, shared_name=None, name=None)\n",
        "        images_batch, labels_batch, image_locations_batch = decodeRGB(input_queue,FLAGS.seq_length,FLAGS.size)\n",
        "        images_batch = tf.to_float(images_batch)\n",
        "        images_batch -= 128.0\n",
        "        images_batch /= 128.0  # scale all pixel values in range: [-1,1]\n",
        "\n",
        "        images_batch = tf.reshape(images_batch,[-1,96,96,3])\n",
        "        labels_batch = tf.reshape(labels_batch,[-1,2])\n",
        "        \n",
        "        if FLAGS.network == 'vggface_4096':\n",
        "            from vggface import vggface_4096x4096x2 as net\n",
        "            network = net.VGGFace(FLAGS.batch_size * FLAGS.seq_length)\n",
        "            network.setup(images_batch)\n",
        "            prediction = network.get_output()\n",
        "            \n",
        "        elif FLAGS.network == 'vggface_2000':\n",
        "            from vggface import vggface_4096x2000x2 as net\n",
        "            network = net.VGGFace(FLAGS.batch_size * FLAGS.seq_length)\n",
        "            network.setup(images_batch)\n",
        "            prediction = network.get_output()\n",
        "        \n",
        "        elif FLAGS.network == 'affwildnet_resnet':\n",
        "            from tensorflow.contrib.slim.python.slim.nets import resnet_v1\n",
        "            with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
        "                net,_  = resnet_v1.resnet_v1_50(inputs=images_batch,is_training=False,num_classes=None)\n",
        "            \n",
        "            with tf.variable_scope('rnn') as scope:\n",
        "                cnn = tf.reshape(net,[FLAGS.batch_size,FLAGS.seq_length,-1])\n",
        "                cell= tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(128) for _ in range(2)])\n",
        "                outputs, _ = tf.nn.dynamic_rnn(cell, cnn, dtype=tf.float32)\n",
        "                outputs = tf.reshape(outputs, (FLAGS.batch_size * FLAGS.seq_length, 128))\n",
        "                        \n",
        "                weights_initializer = tf.truncated_normal_initializer(\n",
        "                    stddev=0.01)\n",
        "                weights = tf.get_variable('weights_output',\n",
        "                                        shape=[128, 2],\n",
        "                                        initializer=weights_initializer,\n",
        "                                        trainable = True)\n",
        "                biases = tf.get_variable('biases_output',\n",
        "                                        shape=[2],\n",
        "                                        initializer=tf.zeros_initializer,trainable = True)\n",
        "                \n",
        "                prediction = tf.nn.xw_plus_b(outputs, weights, biases) \n",
        "\n",
        "        elif FLAGS.network == 'affwildnet_vggface':\n",
        "            from affwildnet import vggface_gru as net\n",
        "            network = net.VGGFace(FLAGS.batch_size, FLAGS.seq_length)\n",
        "            network.setup(images_batch)\n",
        "            prediction = network.get_output()\n",
        "        \n",
        "        num_batches = int(len(image_list)/FLAGS.batch_size)\n",
        "\n",
        "        variables_to_restore =  tf.global_variables()\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "\n",
        "            init_fn = slim.assign_from_checkpoint_fn(\n",
        "                            FLAGS.pretrained_model_checkpoint_path, variables_to_restore,\n",
        "                            ignore_missing_vars=False)\n",
        "\n",
        "            init_fn(sess)\n",
        "            # print('Loading model {}'.format(FLAGS.pretrained_model_checkpoint_path))\n",
        "\n",
        "            tf.train.start_queue_runners(sess=sess)\n",
        "\n",
        "            coord = tf.train.Coordinator()\n",
        "    \n",
        "            evaluated_predictions = []\n",
        "            evaluated_labels = []\n",
        "            images = []\n",
        "\n",
        "            try:\n",
        "                for _ in tqdm(range(num_batches), desc = \"Batch num\"):\n",
        "\n",
        "                    pr, l, imm = sess.run([prediction, labels_batch, image_locations_batch])\n",
        "                    evaluated_predictions.append(pr)\n",
        "                    evaluated_labels.append(l)\n",
        "                    images.append(imm)\n",
        "    \n",
        "                    if coord.should_stop():\n",
        "                        break\n",
        "                coord.request_stop()\n",
        "            except Exception as e:\n",
        "                coord.request_stop(e)\n",
        "\n",
        "            predictions = np.reshape(evaluated_predictions, (-1, 2))\n",
        "            labels = np.reshape(evaluated_labels, (-1, 2))\n",
        "            images = np.reshape(images, (-1))\n",
        "\n",
        "            conc_arousal = concordance_cc2(predictions[:,1], labels[:,1])\n",
        "            conc_valence = concordance_cc2(predictions[:,0], labels[:,0])\n",
        "    \n",
        "            print('Concordance on valence : {}'.format(conc_valence))\n",
        "            print('Concordance on arousal : {}'.format(conc_arousal))\n",
        "            # print('Concordance on total : {}'.format((conc_arousal+conc_valence)/2))\n",
        "\n",
        "            # print(labels[:,1])\n",
        "            # print(labels[:,0])\n",
        "\n",
        "            mse_arousal = sum((predictions[:,1] - labels[:,1])**2)/len(labels[:,1])\n",
        "            print('MSE Arousal : {}'.format(mse_arousal))\n",
        "            mse_valence = sum((predictions[:,0] - labels[:,0])**2)/len(labels[:,0])\n",
        "            print('MSE Valence : {}'.format(mse_valence))\n",
        "\n",
        "    return conc_valence, conc_arousal, (conc_arousal+conc_valence)/2, mse_arousal, mse_valence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtS8He5CKBcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}